{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPU6Zj0bCrZcd/fya8R/DKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tineeee2002/Knowledge-Distillation-on-Graph/blob/main/Knowledge%20Distillation%20on%20Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFp_pahXh-e6",
        "outputId": "91a9d9d8-35e8-40e7-adc8-11d3e5ace5fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOUNNm7FhwMU",
        "outputId": "970adcc4-5f9e-471b-8c66-bb8d319b6754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Teacher Model...\n",
            "Epoch 000, Loss: 1.9468, Train: 0.2500, Val: 0.1760, Test: 0.1830\n",
            "Epoch 020, Loss: 1.9054, Train: 0.8571, Val: 0.6260, Test: 0.6070\n",
            "Epoch 040, Loss: 1.7951, Train: 0.8857, Val: 0.6620, Test: 0.6560\n",
            "Epoch 060, Loss: 1.5959, Train: 0.9000, Val: 0.6700, Test: 0.6790\n",
            "Epoch 080, Loss: 1.2238, Train: 0.9357, Val: 0.7160, Test: 0.7250\n",
            "Epoch 100, Loss: 0.8771, Train: 0.9571, Val: 0.7580, Test: 0.7720\n",
            "Epoch 120, Loss: 0.5771, Train: 0.9786, Val: 0.7780, Test: 0.8020\n",
            "Epoch 140, Loss: 0.3888, Train: 0.9857, Val: 0.7800, Test: 0.8090\n",
            "Epoch 160, Loss: 0.2968, Train: 0.9857, Val: 0.7860, Test: 0.8090\n",
            "Epoch 180, Loss: 0.2216, Train: 0.9857, Val: 0.7880, Test: 0.8080\n",
            "\n",
            "Training Student Model with Distillation...\n",
            "Epoch 000, Loss: 42.9325, Train: 0.2571, Val: 0.1440, Test: 0.1740\n",
            "Task Loss: 1.9464, KD Loss: 3.4547, Structure Loss: 129.7110\n",
            "Epoch 020, Loss: 16.9481, Train: 0.5357, Val: 0.2440, Test: 0.2850\n",
            "Task Loss: 1.7885, KD Loss: 3.4832, Structure Loss: 43.5654\n",
            "Epoch 040, Loss: 10.1971, Train: 0.8643, Val: 0.5180, Test: 0.5570\n",
            "Task Loss: 1.6459, KD Loss: 3.0779, Structure Loss: 22.3484\n",
            "Epoch 060, Loss: 8.1655, Train: 0.9571, Val: 0.6880, Test: 0.7030\n",
            "Task Loss: 1.3091, KD Loss: 2.6033, Structure Loss: 17.6480\n",
            "Epoch 080, Loss: 9.0576, Train: 0.9714, Val: 0.7200, Test: 0.7460\n",
            "Task Loss: 0.9980, KD Loss: 1.7403, Structure Loss: 23.3847\n",
            "Epoch 100, Loss: 7.0432, Train: 0.9714, Val: 0.7720, Test: 0.8030\n",
            "Task Loss: 0.7099, KD Loss: 1.4216, Structure Loss: 18.2679\n",
            "Epoch 120, Loss: 7.4381, Train: 0.9857, Val: 0.7700, Test: 0.7930\n",
            "Task Loss: 0.5226, KD Loss: 1.0074, Structure Loss: 21.0367\n",
            "Epoch 140, Loss: 7.5924, Train: 0.9857, Val: 0.7780, Test: 0.8120\n",
            "Task Loss: 0.4070, KD Loss: 0.8426, Structure Loss: 22.2663\n",
            "Epoch 160, Loss: 8.1907, Train: 0.9857, Val: 0.7800, Test: 0.8000\n",
            "Task Loss: 0.3319, KD Loss: 0.7632, Structure Loss: 24.6697\n",
            "Epoch 180, Loss: 6.4013, Train: 0.9929, Val: 0.7840, Test: 0.8060\n",
            "Task Loss: 0.3130, KD Loss: 0.7502, Structure Loss: 18.7940\n",
            "\n",
            "Final Results:\n",
            "Teacher Model:\n",
            "Train accuracy: 1.0000\n",
            "Val accuracy: 0.7920\n",
            "Test accuracy: 0.8070\n",
            "\n",
            "Student Model:\n",
            "Train accuracy: 0.9929\n",
            "Val accuracy: 0.7840\n",
            "Test accuracy: 0.8110\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "# Load Cora dataset\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora', transform=NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = data.to(device)\n",
        "\n",
        "class TeacherGCN(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes):\n",
        "        super(TeacherGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h1 = F.relu(self.conv1(x, edge_index))\n",
        "        h1 = self.dropout(h1)\n",
        "        h2 = F.relu(self.conv2(h1, edge_index))\n",
        "        h2 = self.dropout(h2)\n",
        "        out = self.conv3(h2, edge_index)\n",
        "        return out, [h1, h2]\n",
        "\n",
        "class StudentGCN(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes):\n",
        "        super(StudentGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h1 = F.relu(self.conv1(x, edge_index))\n",
        "        h1 = self.dropout(h1)\n",
        "        out = self.conv2(h1, edge_index)\n",
        "        return out, [h1]\n",
        "\n",
        "class LocalStructurePreservingDistillation:\n",
        "    def __init__(self, teacher_model, student_model, temperature=4.0):\n",
        "        self.teacher = teacher_model\n",
        "        self.student = student_model\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def get_local_structure(self, node_idx, embeddings, edge_index, num_hops=4):\n",
        "\n",
        "        if isinstance(node_idx, int):  # Nếu node_idx là một giá trị integer (một node duy nhất)\n",
        "            node_idx = torch.tensor([node_idx], dtype=torch.long)  # Chuyển thành tensor 1 chiều\n",
        "        elif node_idx.dim() == 0:  # Nếu node_idx là một scalar (0 chiều)\n",
        "            node_idx = node_idx.unsqueeze(0)  # Chuyển thành tensor 1 chiều\n",
        "\n",
        "        # Kiểm tra rằng node_idx có chiều đúng và edge_index cũng vậy\n",
        "        assert node_idx.dim() == 1, f\"node_idx must be a 1D tensor, got {node_idx.dim()} dimension.\"\n",
        "        assert edge_index.dim() == 2 and edge_index.size(0) == 2, \"edge_index must be a tensor of shape (2, E)\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Lấy k-hop subgraph\n",
        "        subset, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
        "            node_idx, num_hops, edge_index, relabel_nodes=True)\n",
        "        subset = subset.to(torch.long)\n",
        "        # Lấy embeddings của các nodes trong subgraph\n",
        "        local_embeddings = embeddings[subset]\n",
        "\n",
        "        # Tính similarity matrix\n",
        "        sim_matrix = torch.mm(local_embeddings, local_embeddings.t())\n",
        "\n",
        "        return sim_matrix, subset\n",
        "\n",
        "    def compute_distillation_loss(self, x, edge_index, labels, mask):\n",
        "        # Forward pass qua teacher\n",
        "        with torch.no_grad():\n",
        "            teacher_logits, teacher_features = self.teacher(x, edge_index)\n",
        "            teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n",
        "\n",
        "        # Forward pass qua student\n",
        "        student_logits, student_features = self.student(x, edge_index)\n",
        "        student_probs = F.softmax(student_logits / self.temperature, dim=1)\n",
        "\n",
        "        # 1. Knowledge Distillation Loss\n",
        "        kd_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits[mask] / self.temperature, dim=1),\n",
        "            teacher_probs[mask],\n",
        "            reduction='batchmean'\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        # 2. Local Structure Preservation Loss\n",
        "        structure_loss = 0\n",
        "        sampled_nodes = torch.where(mask)[0]\n",
        "        batch_nodes = sampled_nodes[torch.randint(0, len(sampled_nodes), (10,))]\n",
        "\n",
        "        for node_idx in batch_nodes:\n",
        "            # Lấy cấu trúc cục bộ từ teacher và student\n",
        "            teacher_sim, subset = self.get_local_structure(\n",
        "                node_idx, teacher_features[-1], edge_index)\n",
        "            student_sim, _ = self.get_local_structure(\n",
        "                node_idx, student_features[-1], edge_index)\n",
        "\n",
        "            structure_loss += F.mse_loss(student_sim, teacher_sim)\n",
        "\n",
        "        structure_loss = structure_loss / len(batch_nodes)\n",
        "\n",
        "        # 3. Task-specific Loss\n",
        "        task_loss = F.cross_entropy(student_logits[mask], labels[mask])\n",
        "\n",
        "        # Tổng hợp losses\n",
        "        total_loss = task_loss + 0.6 * kd_loss + 0.3 * structure_loss\n",
        "\n",
        "        return total_loss, task_loss, kd_loss, structure_loss\n",
        "\n",
        "def train_teacher(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out, _ = model(data.x, data.edge_index)\n",
        "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "def evaluate(model, data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out, _ = model(data.x, data.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct = pred[mask] == data.y[mask]\n",
        "        acc = int(correct.sum()) / int(mask.sum())\n",
        "    return acc\n",
        "\n",
        "# Khởi tạo models\n",
        "num_features = dataset.num_features\n",
        "hidden_dim = 64\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "teacher = TeacherGCN(num_features, hidden_dim, num_classes).to(device)\n",
        "student = StudentGCN(num_features, hidden_dim, num_classes).to(device)\n",
        "\n",
        "# Train teacher trước\n",
        "teacher_optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
        "best_val_acc = 0\n",
        "best_teacher_state = None\n",
        "\n",
        "print(\"Training Teacher Model...\")\n",
        "for epoch in range(200):\n",
        "    loss = train_teacher(teacher, data, teacher_optimizer)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = evaluate(teacher, data, data.train_mask)\n",
        "        val_acc = evaluate(teacher, data, data.val_mask)\n",
        "        test_acc = evaluate(teacher, data, data.test_mask)\n",
        "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
        "              f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_teacher_state = teacher.state_dict()\n",
        "\n",
        "# Load best teacher model\n",
        "teacher.load_state_dict(best_teacher_state)\n",
        "\n",
        "# Khởi tạo distiller và optimizer cho student\n",
        "distiller = LocalStructurePreservingDistillation(teacher, student)\n",
        "student_optimizer = torch.optim.Adam(student.parameters(), lr=0.01)\n",
        "\n",
        "# Train student với distillation\n",
        "print(\"\\nTraining Student Model with Distillation...\")\n",
        "best_val_acc = 0\n",
        "best_student_state = None\n",
        "\n",
        "for epoch in range(200):\n",
        "    student.train()\n",
        "    total_loss, task_loss, kd_loss, structure_loss = distiller.compute_distillation_loss(\n",
        "        data.x, data.edge_index, data.y, data.train_mask)\n",
        "\n",
        "    student_optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    student_optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = evaluate(student, data, data.train_mask)\n",
        "        val_acc = evaluate(student, data, data.val_mask)\n",
        "        test_acc = evaluate(student, data, data.test_mask)\n",
        "        print(f'Epoch {epoch:03d}, Loss: {total_loss:.4f}, Train: {train_acc:.4f}, '\n",
        "              f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
        "        print(f'Task Loss: {task_loss:.4f}, KD Loss: {kd_loss:.4f}, '\n",
        "              f'Structure Loss: {structure_loss:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_student_state = student.state_dict()\n",
        "\n",
        "# Load best student model và evaluate\n",
        "student.load_state_dict(best_student_state)\n",
        "\n",
        "# Final evaluation\n",
        "print(\"\\nFinal Results:\")\n",
        "print(\"Teacher Model:\")\n",
        "print(f\"Train accuracy: {evaluate(teacher, data, data.train_mask):.4f}\")\n",
        "print(f\"Val accuracy: {evaluate(teacher, data, data.val_mask):.4f}\")\n",
        "print(f\"Test accuracy: {evaluate(teacher, data, data.test_mask):.4f}\")\n",
        "\n",
        "print(\"\\nStudent Model:\")\n",
        "print(f\"Train accuracy: {evaluate(student, data, data.train_mask):.4f}\")\n",
        "print(f\"Val accuracy: {evaluate(student, data, data.val_mask):.4f}\")\n",
        "print(f\"Test accuracy: {evaluate(student, data, data.test_mask):.4f}\")"
      ]
    }
  ]
}